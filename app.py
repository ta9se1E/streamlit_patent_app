import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langgraph.graph import StateGraph
from typing import List, Annotated, Literal, Sequence, TypedDict
from langgraph.graph import END, StateGraph, START
import asyncio
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.schema import Document
from dotenv import load_dotenv
from langsmith import Client
import streamlit_authenticator as stauth
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import japanize_matplotlib 
import numpy as np
import seaborn as sns
from st_login_form import login_form
from datetime import datetime
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud

# ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®æŒ‡å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦åˆ©ç”¨ï¼‰
font_path1 = "./font/NotoSansJP-Regular.ttf"

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv(dotenv_path=".env.example")

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
PASSWORD = os.getenv("PASSWORD")

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ãƒœãƒƒã‚¯ã‚¹
inputText_A = st.text_input('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', type="password")

# ç’°å¢ƒå¤‰æ•°`PASSWORD`ã¨æ¯”è¼ƒ
if inputText_A == PASSWORD:
        
    # ---- ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸå¾Œã«è¡¨ç¤ºã™ã‚‹ã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ã‚’ã“ã“ã«è¨˜è¿° ----
    st.write("ã“ã“ã«ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¢ãƒ¼ãƒ‰ãªã©ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã§ãã¾ã™ã€‚")

    # --- ãƒ¢ãƒ¼ãƒ‰é¸æŠ ---
    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ", "ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ "))

    if mode == "ãƒãƒ£ãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ":
        
        load_dotenv(dotenv_path ="langchain-book/test/.env.example")

        class RouteQuery(BaseModel):
            """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã‚’æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚"""

            datasource: Literal["vectorstore", "web_search"] = Field(
                ...,
                description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¿œã˜ã¦ã€ã‚¦ã‚§ãƒ–æ¤œç´¢ã¾ãŸã¯ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚",
            )


        class GradeDocuments(BaseModel):
            """å–å¾—ã•ã‚ŒãŸæ–‡æ›¸ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã®ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""

            binary_score: str = Field(
                description="æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
            )


        class GradeHallucinations(BaseModel):
            """ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã«ãŠã‘ã‚‹å¹»è¦šã®æœ‰ç„¡ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""

            binary_score: str = Field(
                description="å›ç­”ãŒäº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
            )

        class GradeAnswer(BaseModel):
            """å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""

            binary_score: str = Field(
                description="å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
            )

        class GraphState(TypedDict):
            """
            ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’è¡¨ã—ã¾ã™ã€‚

            å±æ€§:
                question: è³ªå•
                generation: LLMç”Ÿæˆ
                documents: æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
            """

            question: str
            generation: str
            documents: List[str]

        async def route_question(state):
            st.session_state.status.update(label=f"**---ROUTE QUESTION---**", state="running", expanded=True)
            st.session_state.log += "---ROUTE QUESTION---" + "\n\n"
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            structured_llm_router = llm.with_structured_output(RouteQuery)

            system = """ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã¾ãŸã¯ã‚¦ã‚§ãƒ–æ¤œç´¢ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
        ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ã¯åœ§åŠ›å®¹å™¨ã®è£½é€ æ–¹æ³•ã‚„é–¢é€£æŠ€è¡“ã«é–¢é€£ã™ã‚‹ç‰¹è¨±æ–‡æ›¸ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
        ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹è³ªå•ã«ã¯ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½¿ç”¨ã—ã€ãã‚Œä»¥å¤–ã®å ´åˆã¯ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"""
            route_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system),
                    ("human", "{question}"),
                ]
            )

            question_router = route_prompt | structured_llm_router

            question = state["question"]
            source = question_router.invoke({"question": question})
            if source.datasource == "web_search":
                st.session_state.log += "---ROUTE QUESTION TO WEB SEARCH---" + "\n\n"
                st.session_state.placeholder.markdown("---ROUTE QUESTION TO WEB SEARCH---")
                return "web_search"
            elif source.datasource == "vectorstore":
                st.session_state.placeholder.markdown("ROUTE QUESTION TO RAG")
                st.session_state.log += "ROUTE QUESTION TO RAG" + "\n\n"
                return "vectorstore"

        async def retrieve(state):
            st.session_state.status.update(label=f"**---RETRIEVE---**", state="running", expanded=True)
            st.session_state.placeholder.markdown(f"RETRIEVINGâ€¦\n\nKEY WORD:{state['question']}")
            st.session_state.log += f"RETRIEVINGâ€¦\n\nKEY WORD:{state['question']}" + "\n\n"

            
            #ç·¨é›†ãƒã‚¤ãƒ³ãƒˆ
            embd = OpenAIEmbeddings(model="text-embedding-3-small")


            from langchain_community.document_loaders import PyPDFDirectoryLoader
            from langchain.text_splitter import CharacterTextSplitter 
            from langchain.vectorstores import FAISS 
            from langchain.document_loaders import PyPDFLoader 
            
            retriever = FAISS.load_local("vectorstore_20250404", 
                                                    embd,
                                                    allow_dangerous_deserialization=True)
            #ç·¨é›†çµ‚ã‚ã‚Š

            question = state["question"]
            documents = retriever.similarity_search(question)
            
            for doc in documents:
                if "source" in doc.metadata:
                    doc.page_content += f"\n\nSource: {doc.metadata['source']}"
                else:
                    doc.page_content += "\n\nSource: Unknown"

            
            st.session_state.placeholder.markdown("RETRIEVE SUCCESS!!")
            return {"documents": documents, "question": question}

        async def web_search(state):
            st.session_state.status.update(label=f"**---WEB SEARCH---**", state="running", expanded=True)
            st.session_state.placeholder.markdown(f"WEB SEARCHâ€¦\n\nKEY WORD:{state['question']}")
            st.session_state.log += f"WEB SEARCHâ€¦\n\nKEY WORD:{state['question']}" + "\n\n"
            
            TAVILY_API_KEY = os.getenv("tavily_api_key")

            question = state["question"]
            web_search_tool = TavilySearchResults(k=3)

            docs = web_search_tool.invoke({"query": question})
            web_results = []
            
            #è¿½åŠ 
            for doc in docs:
                content_with_source = doc["content"] + f"\n\nSource: {doc.get('source', 'Unknown')}"
                web_results.append(Document(page_content=content_with_source, metadata={"source": doc.get('source', 'Unknown')}))
            
            return {"documents": web_results, "question": question}


        async def grade_documents(state):
            st.session_state.number_trial += 1
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            structured_llm_grader = llm.with_structured_output(GradeDocuments)

            system = """ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å–å¾—ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ„å‘³ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é–¢é€£æ€§ãŒã‚ã‚‹ã¨è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        ç›®çš„ã¯æ˜ã‚‰ã‹ã«èª¤ã£ãŸå–å¾—ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã§ã™ã€‚å³å¯†ãªãƒ†ã‚¹ãƒˆã§ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚"""
            grade_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system),
                    ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
                ]
            )

            retrieval_grader = grade_prompt | structured_llm_grader
            st.session_state.status.update(label=f"**---CHECK DOCUMENT RELEVANCE TO QUESTION---**", state="running", expanded=False)
            st.session_state.log += "**---CHECK DOCUMENT RELEVANCE TO QUESTION---**" + "\n\n"
            question = state["question"]
            documents = state["documents"]
            filtered_docs = []
            i = 0
            for d in documents:
                if st.session_state.number_trial <= 2:
                    file_name = d.metadata["source"]
                    file_name = os.path.basename(file_name.replace("\\","/"))
                    i += 1
                    score = retrieval_grader.invoke(
                        {"question": question, "document": d.page_content}
                    )
                    grade = score.binary_score
                    if grade == "yes":
                        st.session_state.status.update(label=f"**---GRADE: DOCUMENT RELEVANT---**", state="running", expanded=True)
                        st.session_state.placeholder.markdown(f"DOC {i}/{len(documents)} : **RELEVANT**\n\n")
                        st.session_state.log += "---GRADE: DOCUMENT RELEVANT---" + "\n\n"
                        st.session_state.log += f"doc {i}/{len(documents)} : RELEVANT\n\n"
                        filtered_docs.append(d)
                    else:
                        st.session_state.status.update(label=f"**---GRADE: DOCUMENT NOT RELEVANT---**", state="error", expanded=True)
                        st.session_state.placeholder.markdown(f"DOC {i}/{len(documents)} : **NOT RELEVANT**\n\n")
                        st.session_state.log += "---GRADE: DOCUMENT NOT RELEVANT---" + "\n\n"
                        st.session_state.log += f"DOC {i}/{len(documents)} : NOT RELEVANT\n\n"
                else:

                    filtered_docs.append(d)

            if not st.session_state.number_trial <= 2:
                st.session_state.status.update(label=f"**---NO NEED TO CHECK---**", state="running", expanded=True)
                st.session_state.placeholder.markdown("QUERY TRANSFORMATION HAS BEEN COMPLETED")
                st.session_state.log += "QUERY TRANSFORMATION HAS BEEN COMPLETED" + "\n\n"

            return {"documents": filtered_docs, "question": question}

        async def generate(state):
            st.session_state.status.update(label=f"**---GENERATE---**", state="running", expanded=False)
            st.session_state.log += "---GENERATE---" + "\n\n"
            prompt = ChatPromptTemplate.from_messages(
                    [
                        ("system", """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚è€ƒã«è³ªå•ã«å¯¾ã—ç­”ãˆã¦ä¸‹ã•ã„ã€‚"""),
                        ("human", """Question: {question} 
        Context: {context}"""),
                    ]
                )
                
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

            rag_chain = prompt | llm | StrOutputParser()
            question = state["question"]
            documents = state["documents"]
            generation = rag_chain.invoke({"context": documents, "question": question})
            return {"documents": documents, "question": question, "generation": generation}


        async def transform_query(state):
            st.session_state.status.update(label=f"**---TRANSFORM QUERY---**", state="running", expanded=True)
            st.session_state.placeholder.empty()
            st.session_state.log += "---TRANSFORM QUERY---" + "\n\n"
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

            system = """ã‚ãªãŸã¯ã€å…¥åŠ›ã•ã‚ŒãŸè³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¤œç´¢ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚ˆã‚Šè‰¯ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¤‰æ›ã™ã‚‹è³ªå•ãƒªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
        è³ªå•ã‚’è¦‹ã¦ã€è³ªå•è€…ã®æ„å›³/æ„å‘³ã«ã¤ã„ã¦æ¨è«–ã—ã¦ã‚ˆã‚Šè‰¯ã„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç‚ºã®è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""
            re_write_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system),
                    (
                        "human",
                        "Here is the initial question: \n\n {question} \n Formulate an improved question.",
                    ),
                ]
            )

            question_rewriter = re_write_prompt | llm | StrOutputParser()
            question = state["question"]
            documents = state["documents"]
            better_question = question_rewriter.invoke({"question": question})
            st.session_state.log += f"better_question : {better_question}\n\n"
            st.session_state.placeholder.markdown(f"better_question : {better_question}")
            return {"documents": documents, "question": better_question}


        async def decide_to_generate(state):
            filtered_documents = state["documents"]
            if not filtered_documents:
                st.session_state.status.update(label=f"**---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---**", state="error", expanded=False)
                st.session_state.log += "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---" + "\n\n"
                return "transform_query"                                     
            else:
                st.session_state.status.update(label=f"**---DECISION: GENERATE---**", state="running", expanded=False)
                st.session_state.log += "---DECISION: GENERATE---" + "\n\n"
                return "generate"

        async def grade_generation_v_documents_and_question(state):
            st.session_state.number_trial += 1
            st.session_state.status.update(label=f"**---CHECK HALLUCINATIONS---**", state="running", expanded=False)
            st.session_state.log += "---CHECK HALLUCINATIONS---" + "\n\n"
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            structured_llm_grader = llm.with_structured_output(GradeHallucinations)

            system = """ã‚ãªãŸã¯ã€LLMã®ç”ŸæˆãŒå–å¾—ã•ã‚ŒãŸäº‹å®Ÿã®ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹/ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
        ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚ã€Œyesã€ã¯ã€å›ç­”ãŒäº‹å®Ÿã®ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹/ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"""
            hallucination_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system),
                    ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
                ]
            )
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            structured_llm_grader = llm.with_structured_output(GradeAnswer)

            system = """ã‚ãªãŸã¯ã€å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹/è§£æ±ºã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
        ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚ã€Œyesã€ã¯ã€å›ç­”ãŒè³ªå•ã‚’è§£æ±ºã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"""
            answer_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system),
                    ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
                ]
            )

            answer_grader = answer_prompt | structured_llm_grader
            hallucination_grader = hallucination_prompt | structured_llm_grader
            question = state["question"]
            documents = state["documents"]
            generation = state["generation"]
            score = hallucination_grader.invoke(
                {"documents": documents, "generation": generation}
            )
            grade = score.binary_score
            if st.session_state.number_trial <= 3:
                if grade == "yes":
                    st.session_state.placeholder.markdown("DECISION: ANSWER IS BASED ON A SET OF FACTS")
                    st.session_state.log += "---DECISION: ANSWER IS BASED ON A SET OF FACTS---" + "\n\n"
                    st.session_state.log += "---GRADE GENERATION vs QUESTION---" + "\n\n"
                    score = answer_grader.invoke({"question": question, "generation": generation})
                    grade = score.binary_score
                    st.session_state.status.update(label=f"**---GRADE GENERATION vs QUESTION---**", state="running", expanded=True)
                    if grade == "yes":
                        st.session_state.status.update(label=f"**---DECISION: GENERATION ADDRESSES QUESTION---**", state="running", expanded=True)
                        with st.session_state.placeholder:
                            st.markdown("**USEFUL!!**")
                            st.markdown(f"question : {question}")
                            st.markdown(f"generation : {generation}")                   
                            st.session_state.log += "---DECISION: GENERATION ADDRESSES QUESTION---" + "\n\n"
                            st.session_state.log += f"USEFUL!!\n\n"
                            st.session_state.log += f"question:{question}\n\n"
                            st.session_state.log += f"generation:{generation}\n\n"
                        return "useful"
                    else:
                        st.session_state.number_trial -= 1
                        st.session_state.status.update(label=f"**---DECISION: GENERATION DOES NOT ADDRESS QUESTION---**", state="error", expanded=True)
                        with st.session_state.placeholder:
                            st.markdown("**NOT USEFUL**")
                            st.markdown(f"question:{question}")
                            st.markdown(f"generation:{generation}")
                            st.session_state.log += "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---" + "\n\n"
                            st.session_state.log += f"NOT USEFUL\n\n"
                            st.session_state.log += f"question:{question}\n\n"
                            st.session_state.log += f"generation:{generation}\n\n"
                        return "not useful"
                else:
                    st.session_state.status.update(label=f"**---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---**", state="error", expanded=True)
                    with st.session_state.placeholder:
                        st.markdown("not grounded")
                        st.markdown(f"question:{question}")
                        st.markdown(f"generation:{generation}")
                        st.session_state.log += "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---" + "\n\n"
                        st.session_state.log += f"not grounded\n\n"
                        st.session_state.log += f"question:{question}\n\n"
                        st.session_state.log += f"generation:{generation}\n\n"
                    return "not supported"
            else:
                st.session_state.status.update(label=f"**---NO NEED TO CHECK---**", state="running", expanded=True)
                st.session_state.placeholder.markdown("TRIAL LIMIT EXCEEDED")
                st.session_state.log += "---NO NEED TO CHECK---" + "\n\n"
                st.session_state.log += "TRIAL LIMIT EXCEEDED" + "\n\n"
                return "useful"
            
        # åˆæœŸåŒ–â†’è¿½åŠ 
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []    

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ›´æ–°â†’è¿½åŠ 
        async def run_workflow(inputs):
            st.session_state.number_trial = 0
            with st.status(label="**GO!!**", expanded=True, state="running") as st.session_state.status:
                st.session_state.placeholder = st.empty()
                value = await st.session_state.workflow.ainvoke(inputs)
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
            st.session_state.chat_history.append({
                "question": inputs["question"],
                "response": value["generation"],
                "documents": value["documents"]  # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            })

            st.session_state.placeholder.empty()
            st.session_state.message_placeholder = st.empty()
            st.session_state.status.update(label="**FINISH!!**", state="complete", expanded=False)
            st.session_state.message_placeholder.markdown(value["generation"])
            with st.popover("ãƒ­ã‚°"):
                st.markdown(st.session_state.log)

        if st.button("Show Chat History"):
            for i, entry in enumerate(st.session_state.chat_history):
                st.markdown(f"### Message {i+1}")
                st.markdown(f"**Question:** {entry['question']}")
                st.markdown(f"**Response:** {entry['response']}")
                st.markdown("**Sources:**")
                for doc in entry["documents"]:
                    source_info = doc.metadata.get("source", "Unknown")
                    st.markdown(f"- {source_info}")

        def st_rag_langgraph():
            
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
            LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
            os.environ["LANGCHAIN_PROJECT"] =  "Carbon-GPTs"

            client = Client(api_key=LANGSMITH_API_KEY)

            if 'log' not in st.session_state:
                st.session_state.log = ""

            if 'status_container' not in st.session_state:
                st.session_state.status_container = st.empty()

            if not hasattr(st.session_state, "workflow"):

                workflow = StateGraph(GraphState)

                workflow.add_node("web_search", web_search)
                workflow.add_node("retrieve", retrieve)
                workflow.add_node("grade_documents", grade_documents)
                workflow.add_node("generate", generate)
                workflow.add_node("transform_query", transform_query)

                workflow.add_conditional_edges(
                    START,
                    route_question,
                    {
                        "vectorstore": "retrieve",
                        "web_search": "web_search",
                    },
                )
                workflow.add_edge("web_search", "generate")
                workflow.add_edge("retrieve", "grade_documents")
                workflow.add_conditional_edges(
                    "grade_documents",
                    decide_to_generate,
                    {
                        "transform_query": "transform_query",
                        "generate": "generate",
                    },
                )
                workflow.add_edge("transform_query", "retrieve")
                workflow.add_conditional_edges(
                    "generate",
                    grade_generation_v_documents_and_question,
                    {
                        "not supported": "generate",
                        "useful": END,
                        "not useful": "transform_query",
                    },
                )

                app = workflow.compile()
                app = app.with_config(recursion_limit=10,run_name="Agent",tags=["Agent"])
                app.name = "Agent"
                st.session_state.workflow = app


            st.title("Adaptive RAG by LangGraph")

            if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
                st.session_state.log = ""
                with st.chat_message("user", avatar="ğŸ˜Š"):
                    st.markdown(prompt)

                inputs = {"question": prompt}
                asyncio.run(run_workflow(inputs))
    
        if __name__ == "__main__":
            st_rag_langgraph()
        

    elif mode == "ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ ":
        st.title("ãƒ‡ãƒ¼ã‚¿è§£æã‚·ã‚¹ãƒ†ãƒ ")
        
        def st_rag_langgraph():
            st.write("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["csv"])
        
            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file, index_col=0, header=0)
                df["å‡ºé¡˜æ—¥"] = pd.to_datetime(df["å‡ºé¡˜æ—¥"], errors='coerce')
                df["å…¬é–‹ãƒ»å…¬è¡¨æ—¥"] = pd.to_datetime(df["å…¬é–‹ãƒ»å…¬è¡¨æ—¥"], errors='coerce')
                df["å‡ºé¡˜å¹´"] = df["å‡ºé¡˜æ—¥"].dt.year
                df["å…¬é–‹ãƒ»å…¬è¡¨å¹´"] = df["å…¬é–‹ãƒ»å…¬è¡¨æ—¥"].dt.year
                df["å‡ºé¡˜ä»¶æ•°"] = df["å‡ºé¡˜ç•ªå·"].value_counts().values
                
                start_date = datetime(2000, 1, 1)
                end_date = datetime(2025, 3, 31)
                
                term = st.slider("æœŸé–“",value=[start_date,end_date])
                
                _df = df[(df["å‡ºé¡˜æ—¥"] >= term[0]) & (df["å‡ºé¡˜æ—¥"] <= term[1])]
                
                st.write("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:", _df.head())
                
               
                analyze_mode = st.sidebar.radio("åˆ†æãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("ç‰¹è¨±å‡ºé¡˜ä»¶æ•°æŠŠæ¡", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ","ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—"))
                
                if analyze_mode == "ç‰¹è¨±å‡ºé¡˜ä»¶æ•°æŠŠæ¡":
                    _ = pd.DataFrame(_df.groupby(["å‡ºé¡˜äººãƒ»æ¨©åˆ©è€…(æœ€æ–°)","å‡ºé¡˜å¹´"]).agg({"å‡ºé¡˜ä»¶æ•°":"sum", "Unnamed: 8":"first"}))
                    fig, ax = plt.subplots(figsize=(25,12))
                    sns.barplot(data=_, x="å‡ºé¡˜å¹´", y="å‡ºé¡˜ä»¶æ•°", hue="Unnamed: 8", errorbar=None)
                    ax.set_xlabel("å‡ºé¡˜å¹´", fontsize=40)
                    ax.set_ylabel("å‡ºé¡˜ä»¶æ•°", fontsize=40)
                    ax.tick_params(axis='y', labelsize=15)
                    ax.tick_params(axis='x', labelsize=15)
                    st.pyplot(fig)
                    
                elif analyze_mode == "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ":
                                        
                    # æ¬ æå€¤ã‚’å‰Šé™¤
                    df_clean = _df.dropna(subset=["å‡ºé¡˜äººãƒ»æ¨©åˆ©è€…(æœ€æ–°)"])

                    # å‡ºé¡˜äººãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆå…±åŒå‡ºé¡˜ã®å ´åˆã€";"ã§åˆ†å‰²ï¼‰
                    edges = []
                    for applicants in df_clean["å‡ºé¡˜äººãƒ»æ¨©åˆ©è€…(æœ€æ–°)"]:
                        applicant_list = [a.strip() for a in applicants.split(";")]
                        if len(applicant_list) > 1:  # å…±åŒå‡ºé¡˜ãŒã‚ã‚‹å ´åˆã®ã¿
                            for i in range(len(applicant_list)):
                                for j in range(i + 1, len(applicant_list)):
                                    edges.append((applicant_list[i], applicant_list[j]))

                    # ã‚°ãƒ©ãƒ•ä½œæˆ
                    G = nx.Graph()
                    G.add_edges_from(edges)

                    # æç”»
                    plt.figure(figsize=(10, 7))
                    pos = nx.spring_layout(G, seed=42)  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
                    nx.draw(G, pos, with_labels=True, node_color="lightblue", edge_color="gray", node_size=2000, font_size=10,
                        font_family="IPAexGothic"  # æ—¥æœ¬èªå¯¾å¿œãƒ•ã‚©ãƒ³ãƒˆã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                        )
                    plt.title("å‡ºé¡˜äººãƒ»å…±åŒå‡ºé¡˜ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", fontsize=14)
                    # Streamlitã§ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                    st.pyplot(plt)
                    
                    df_clean = _df.dropna(subset=["ç™ºæ˜è€…ã¾ãŸã¯è€ƒæ¡ˆè€…(æœ€æ–°)"])

                    # å‡ºé¡˜äººãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆå…±åŒå‡ºé¡˜ã®å ´åˆã€";"ã§åˆ†å‰²ï¼‰
                    edges = []
                    for applicants in df_clean["ç™ºæ˜è€…ã¾ãŸã¯è€ƒæ¡ˆè€…(æœ€æ–°)"]:
                        applicant_list = [a.strip() for a in applicants.split(";")]
                        if len(applicant_list) > 1:  # å…±åŒå‡ºé¡˜ãŒã‚ã‚‹å ´åˆã®ã¿
                            for i in range(len(applicant_list)):
                                for j in range(i + 1, len(applicant_list)):
                                    edges.append((applicant_list[i], applicant_list[j]))

                    # ã‚°ãƒ©ãƒ•ä½œæˆ
                    G = nx.Graph()
                    G.add_edges_from(edges)

                    # æç”»
                    plt.figure(figsize=(10, 7))
                    pos = nx.spring_layout(G, seed=42)  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
                    nx.draw(G, pos, with_labels=True, node_color="lightblue", edge_color="gray", node_size=2000, font_size=10,
                        font_family="IPAexGothic"  # æ—¥æœ¬èªå¯¾å¿œãƒ•ã‚©ãƒ³ãƒˆã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
                        )
                    plt.title("ç™ºæ˜äººãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", fontsize=14)
                    # Streamlitã§ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º
                    st.pyplot(plt)
                    
                if analyze_mode == "ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—":
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç™ºæ˜ç­‰ã®åç§°ã‚’å–å¾—
                    appnames = _df['ç™ºæ˜ç­‰ã®åç§°'].values

                    # Janomeãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’åˆæœŸåŒ–
                    tokenizer = Tokenizer()
                    words = []

                    # åè©ã®é€£ç¶šã‚’æ¤œå‡ºã—ã¦è¤‡åˆåè©ã¨ã—ã¦çµåˆ
                    for appname in appnames:
                        tokens = tokenizer.tokenize(appname)
                        noun_phrase = []  # è¤‡åˆåè©ç”¨ã®ãƒªã‚¹ãƒˆ
                        for token in tokens:
                            if token.part_of_speech.startswith('åè©'):
                                noun_phrase.append(token.surface)  # åè©ã‚’è¿½åŠ 
                            else:
                                if noun_phrase:  # åè©ãŒé€£ç¶šã—ã¦ã„ãŸå ´åˆã€çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                    words.append("".join(noun_phrase))
                                    noun_phrase = []  # ãƒªã‚»ãƒƒãƒˆ
                        # æœ€å¾Œã«æ®‹ã£ãŸåè©ã‚’è¿½åŠ 
                        if noun_phrase:
                            words.append("".join(noun_phrase))

                    # å˜èªã®å‡ºç¾é »åº¦ã‚’è¨ˆç®—
                    df_words = pd.Series(words).value_counts()
                    word_counts = df_words.to_dict()

                    # å˜èªã®é »åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                    fig,ax = plt.subplots(figsize=(20, 13))
                    head_20 = df_words.iloc[:20].copy()
                    ax.barh(y=head_20.index, width=head_20.values, color="orange")
                                        
                    # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
                    ax.set_title("é »å‡ºå˜èªã®ãƒˆãƒƒãƒ—20", fontsize=40)
                    ax.set_xlabel("é »åº¦", fontsize=40)
                    ax.set_ylabel("å˜èª", fontsize=40)

                    # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’èª¿æ•´
                    #ax.set_yticks(head_20.index)
                    #ax.set_yticklabels(head_20.index, rotation=0, fontsize=30)
                    ax.tick_params(axis='y', labelsize=30)
                    ax.tick_params(axis='x', labelsize=30)
                    
                    st.pyplot(fig)
                    
                    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®š
                    font_path1 = "./font/NotoSansJP-Regular.ttf"  # é©åˆ‡ãªãƒ‘ã‚¹ã‚’æŒ‡å®š

                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
                    wordcloud = WordCloud(
                        background_color='white', 
                        width=800, 
                        height=600, 
                        font_path=font_path1
                    )

                    wordcloud.generate_from_frequencies(word_counts)

                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º
                    plt.figure(figsize=(10, 8))
                    plt.imshow(wordcloud, interpolation='bilinear')
                    plt.axis('off')

                    # Streamlitã§è¡¨ç¤º
                    st.pyplot(plt)
            
                
        if __name__ == "__main__":
            st_rag_langgraph()


